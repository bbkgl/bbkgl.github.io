---
layout:     post
title:      使用和配置hive碰到的坑
subtitle:   Hadoop/Hive
date:       2019-10-30
author:     bbkgl
header-img: img/post-bg-0001.jpg
catalog: true
tags:
    - Hadoop
---

>只愁歌舞散
>
>化作彩云飞

贝老师的课《大数据存储与处理》的大作业是使用Hadoop和Hive进行数据分析，这里来记录下碰到的坑。

## 安装Hadoop

关于安装Hadoop，这里强烈建议参考厦门大学数据库实验室的博客[Hadoop安装配置简略教程](<http://dblab.xmu.edu.cn/blog/install-hadoop-simplify/#more-94>)，我感觉是问题不大的。

## 配置Hive

> 参考[实验手册——搜狗搜索日志分析系统](https://www.cnblogs.com/biehongli/p/8074642.html)

####  连接不上hadoop

```shell
mkdir: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
mkdir: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
put: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
put: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
```

这种一般是Hadoop各个结点服务没有成功启动，重新启动即可。

```shell
stop-all.sh
start-all.sh
```

#### hadoop安全模式

显示hadoop处于安全模式，不能进行任何写入操作：

```shell
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/hive. Name node is in safe mode.
Resources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off. NamenodeHostName:localhost
```

如果去搜的话，大部分文章都是提供这么两个解决办法：

1. 等，等hadoop退出安全模式
2.  强制退出安全模式`hadoop/hdfs dfsadmin -safemode leave`

如果足够幸运的话，能够通过以上两种方式解决。

不幸运的话，可以再仔细看看报错信息，**Resources are low on NN**，也就是资源不足。。。

于是搜这个加粗的内容，得到的答案基本都是扩容，毕竟报的是磁盘不足。

当然了，如果发现磁盘空间充足的话，可以考虑这个办法，**重新格式化节点**。

对，重新执行以下命令：

```shell
hdfs namenode -format
start-dfs.sh
start-yarn.sh
```

然后就搞定了，可以添加文件了。。。我也不知道为什么，反正就是可以了。。。

#### mysql-connector-java和mysql版本不匹配

错误：

```shell
FAILED: SemanticException Unable to fetch table sogou_ext. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'OPTION SQL_SELECT_LIMIT=DEFAULT' at line 1
```

之前我用的mysql是5.7，然后mysql-connector-java用的是mysql-connector-java-5.1.9.jar。。。看各种博客上说5.7和5.1是匹配的，结果打脸了呀。

于是我去[官网](<http://central.maven.org/maven2/mysql/mysql-connector-java/8.0.11/>)下载了mysql-connector-java-8.0.jar，完美解决！！！

#### 如果出现文件不能写入

错误：

```shell
put: File /sogou/data_ext._COPYING_ could only be written to 0 of the 1 minReplication nodes. There are 0 datanode(s) running and 0 node(s) are excluded in this operation.
```

考虑两种可能：

- 安全模式，解决如上
- 重复数据，需要重新进行格式化

第二种的话按如下操作：

```shell
sudo rm -rf hadoop/tmp/*
stop-all.sh
hdfs namenode -format
start-dfs.sh
start-yarn.sh
```

## 操作步骤

#### 基本操作

```sql
# 查看数据库
show databases;
# 创建数据库
create database sogou;
#　使用数据库
use sogou;
# 查看所有表名
show tables;
# 创建外部表,使用相对路径绝对URI， # 创建数据库要小心关键字冲突，不能使用date，order,user等关键字
# 还有就是location中，如果是单机要使用lcoalhos而不是master
create external table sogou.sogou_table(ts string, uid string, keyword string, rank int, sorder int, url string) comment 'this is the sogou search data' Row FORMAT DELIMITED  FIELDS TERMINATED BY '\t' stored as TEXTFILE  location 'hdfs://localhost:9000/sogou/data';
```

#### 创建带时间分区的表

创建扩展数据的外部表：

```sql
create external table sogou.sogou_ext(ts string,uid string,keyword string,rank int,sorder int,url string,year int,month int,day int,hour int)row format delimited fields terminated by '\t' stored as textfile location 'hdfs://localhost:9000/sogou_ext/data';
```



创建扩展 4 个字段（年、月、日、小时）数据的外部表，其实就是加了句`partitioned by (year INT,month INT,day INT,hour INT)`：

```sql
create external table sogou.sogou_partition(ts string, uid string, keyword string, rank int, sorder int, url string) comment 'this is the sogou search data' partitioned by (year INT,month INT,day INT,hour INT) Row FORMAT DELIMITED FIELDS TERMINATED BY '\t' stored as TEXTFILE;
```

灌入数据：

```sql
# 设置动态分区。nonstrict全分区字段是动态的
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table sogou.sogou_partition partition(year,month,day,hour) select * from sogou.sogou_ext;
```

简单查询：

```sql
select * from sogou_ext limit 10;
select keyword, count(*) as cnt from sogou.sogou_ext group by keyword order by cnt desc limit 30;

# 结果
百度	38437
baidu	18312
人体艺术	14474
4399小游戏	11438
qq空间	10316
优酷	10158
新亮剑	9654
馆陶县县长闫宁的父亲	9127
公安卖萌	8192
百度一下 你就知道	7504
百度一下	7104
4399	7041
魏特琳	6665
qq网名	6148
7k7k小游戏	5985
黑狐	5610
儿子与母亲不正当关系	5496
新浪微博	5369
李宇春体	5310
新疆暴徒被击毙图片	4997
hao123	4834
123	4829
4399洛克王国	4112
qq头像	4085
nba	4027
龙门飞甲	3917
qq个性签名	3880
张去死	3847
cf官网	3729
凰图腾	3632
```

效果如下：

![5dba79a4bd461d945a7adf4b](https://raw.githubusercontent.com/bbkgl/bbkgl.github.io/master/cloud_img/5dba79a4bd461d945a7adf4b.jpg)

```sql
# 查询访问频次最高的10条网址
select url, count(*) as cnt, count(*)/4999877 from sogou.sogou_ext group by url order by cnt desc limit 10;

# 真实，且还都是http协议

http://www.baidu.com/	73732	0.014746762770364151
http://www.4399.com/	19015	0.00380309355610148
http://www.hao123.com/	14338	0.0028676705446953996
http://www.youku.com/	14085	0.002817069299904778
http://qzone.qq.com/	12920	0.002584063567963772
http://www.7k7k.com/	8326	0.0016652409649277372
http://weibo.com/	7547	0.001509437132153451
http://cf.qq.com/	7544	0.001508837117393088
http://www.xixiwg.com/	7043	0.0014086346524124493
http://www.12306.cn/	6961	0.0013922342489625244
```

![5dba7ec0bd461d945a7b1e5f](https://raw.githubusercontent.com/bbkgl/bbkgl.github.io/master/cloud_img/5dba7ec0bd461d945a7b1e5f.jpg)

后面数据分析应该也是基于**sogou.sogou_ext**这个表。
