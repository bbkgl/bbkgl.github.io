---
layout:     post
title:      使用和配置hive碰到的坑
subtitle:   Hadoop/Hive
date:       2019-10-26
author:     bbkgl
header-img: img/post-bg-0001.jpg
catalog: true
tags:
    - Hadoop
---

>只愁歌舞散
>
>化作彩云飞

贝老师的课《大数据存储与处理》的大作业是使用Hadoop和Hive进行数据分析，这里来记录下碰到的坑。

## 安装Hadoop

关于安装Hadoop，这里强烈建议参考厦门大学数据库实验室的博客[Hadoop安装配置简略教程](<http://dblab.xmu.edu.cn/blog/install-hadoop-simplify/#more-94>)，我感觉是问题不大的。

## 配置Hive

> 参考[实验手册——搜狗搜索日志分析系统](https://www.cnblogs.com/biehongli/p/8074642.html)

####  连接不上hadoop

```shell
mkdir: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
mkdir: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
put: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
put: Call From bbkgl/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
```

这种一般是Hadoop各个结点服务没有成功启动，重新启动即可。

```shell
stop-all.sh
start-all.sh
```

#### hadoop安全模式

显示hadoop处于安全模式，不能进行任何写入操作：

```shell
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/hive. Name node is in safe mode.
Resources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off. NamenodeHostName:localhost
```

如果去搜的话，大部分文章都是提供这么两个解决办法：

1. 等，等hadoop退出安全模式
2.  强制退出安全模式`hadoop/hdfs dfs -safemode leave`

如果足够幸运的话，能够通过以上两种方式解决。

不幸运的话，可以再仔细看看报错信息，**Resources are low on NN**，也就是资源不足。。。

于是搜这个加粗的内容，得到的答案基本都是扩容，毕竟报的是磁盘不足。

当然了，如果发现磁盘空间充足的话，可以考虑这个办法，**重新格式化节点**。

对，重新执行以下命令：

```shell
hdfs namenode -format
start-dfs.sh
start-yarn.sh
```

然后就搞定了，可以添加文件了。。。我也不知道为什么，反正就是可以了。。。